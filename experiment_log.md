遮挡判定有问题，除了看LOS到障碍物的距离还要看障碍物的投影是否在两点中间。

到障碍物的距离判定有问题，应该只看平面距离，因为有的时候chaser的高度不是2m。

目标随机速度接近0的那些随机任务一个episode都走完了还没怎么动，这样学不到什么, 将目标的随机速度改为[0.1, 0.2]。

绕不过第二个柱子，调小速度惩罚系数。

遮挡时直接重置，episode length升不上去，无法收敛。

2024-10-24_11-18-36 将目标的随机角速度范围缩小为(1.4, 1.6)，不再随机正反转。修改奖励函数，使之保持在(-1,1)范围内。

2024-10-25_10-25-30 调超参数：折扣系数从0.99改为0.998，相当于考虑500步内的奖励，当前步长为0.05s，因此相当于考虑未来25s内的奖励。学习率从1e-3改为5e-4，因为之前振荡太严重。接着上一个训练，本次效果有好转，但仍然绕不过第二个柱子。

2024-10-25_11-39-32 算法从SAC换成PPO，环境数可加到16384。

2024-10-25_17-14-01 将replay buffer大小从100K增加到1M，由于显存限制，环境数从512减少到64。

2024-10-26_21-17-54 开启skrl自带的状态、值函数目标归一化，从源码看只应用了state_preprocessor，没有用上value_preprocessor

碰撞检测逻辑应为：过去decimation个substep中如果发生过碰撞则重置，记得重置碰撞标记

/home/sbw/IsaacLab/source/extensions/omni.isaac.lab/omni/isaac/lab/sensors/ray_caster/patterns/patterns.py: 156写错了，应该加1

加入避障用的引导奖励

把vx和vyz分离

避障奖励系数0.3->0.1

动作奖励系数0.001->0.01

网络[256, 256] -> [400, 300]初始化换成kaiming_uniform

初始化kaiming_normal

SAC_2024-11-22_15-49-50 训练evader，无障碍物

SAC_2024-11-22_19-24-21 训练evader，障碍度密度10

SAC_2024-11-23_18-16-45 训练evader，障碍度密度20

SAC_2024-11-26_14-13-51 训练evader，碰撞时重置

SAC_2024-11-26_19-10-21 训练chaser，障碍物密度20，追踪失败时不重置，能学到保持高度和避障

SAC_2024-11-27_13-57-31 训练chaser，目标距离超过2m时重置，奖励比之前更高

SAC_2024-11-27_21-29-48 训练chaser，接着上一个继续

SAC_2024-11-28_15-59-41 训练chaser，上一个奖励看起来还能升，接着上一个继续

SAC_2024-12-02_17-30-28 观测加入旋转矩阵，网络激活函数改为ReLU，隐藏层改为[400, 300]，奖励不能收敛

PPO_2024-12-03_21-51-20 用PPO，rollout=200，batch_size=2048，只在一个地形中训练

PPO_2024-12-04_09-54-22 障碍物个数从2000增加到4000，策略网络中的动作方差（的对数）改成独立参数，初始值为0.5

PPO_2024-12-04_16-00-07 障碍物个数改回2000，地图大小从(200,100)改为(100,100)，相当于障碍物密度不变，从-50m处随机起点，期望距离改成1.0m，训练中出现evader out of range

PPO_2024-12-04_16-31-03 训练evader

PPO_2024-12-04_20-26-52 用新的evader训练chaser，和PPO_2024-12-04_16-00-07配置相同

PPO_2024-12-11_12-11-05 训练evader，用控制器控制加速度

PPO_2024-12-11_19-21-20 用新的evader训练chaser，修改了地图大小但是没有修改障碍物数量导致障碍物密度变为2倍，训练失败

PPO_2024-12-12_11-08-34 修正障碍物密度，重新训练chaser

TransformerStudent_2024-12-18_15-11-15 修正了之前RGB没有归一化的错误，用两个ResNet18分别提取rgb和depth，这个用的数据集中depth截断到10.0

VAE_2024-12-19_09-32-20 训练了VAE，将(4, 224, 224)的RGBD编码成(256, 14, 14)，注意用的数据集中depth进行了 1 / (1 + x) 变换, 数据集中segmentation图像边缘可能出现chaser的螺旋桨

修改了forest1, 现在chaser的螺旋桨不会被分割, 在新的数据上测试VAE_2024-12-19_09-32-20, 还能用

TransformerVAEStudent_2024-12-19_21-07-51 冻结VAE训练Transformer, 一张图片196个token, 序列长度达到了1970, d_model减到256, 用的L1 Loss, 位置和时间编码用方案一

TransformerVAEStudent_2024-12-19_21-38-24 实际上训练的是TransformerStudentSmall, 用EfficientNet-b0处理RGBD, 一张图片一个token, 位置编码改成rgbd和state接续而非一样

TransformerStudent_2024-12-20_21-44-50 ResNet18从头训练, RGBD和在一起, 用MSE loss

TransformerStudent_2024-12-20_21-46-38 和上面一样, 除了改用l1 loss

TransformerStudent_2024-12-26_15-18-54 在policy数据集上训练, 把深度改回截断, 用l1 loss, rgbd合并, 能超过TransformerStudent_2024-12-23_10-48-43吗?

TransformerStudent_2024-12-26_16-14-52 在policy数据集上训练, 用TCN模型, l1 loss, rgbd和state相加

重新训练evader:

PPO_2025-01-13_11-50-16 仿真频率100Hz, 动作周期0.1s, episode length 60s

PPO_2025-01-14_19-43-00 仿真频率120Hz, 动作周期0.05s, episode length 120s

PPO_2025-01-15_10-22-10 仿真频率100Hz, 动作周期0.1s, episode length 120s, 从PPO_2025-01-13_11-50-16继续
	
PPO_2025-01-16_17-22-30 仿真频率100Hz, 动作周期0.1s, episode length 180s, 从PPO_2025-01-15_10-22-10继续

训练chaser:

PPO_2025-01-15_22-52-37 动作周期0.1s, 隐藏层400-400-400并加LayerNorm

PPO_2025-01-15_22-54-14 动作周期0.1s, 隐藏层400-400

PPO_2025-01-16_11-37-42 动作周期0.05s

加入chaser初始随机姿态和距离, 初始时向前0.5m/s速度

PPO_2025-01-16_21-45-27 动作周期0.1s, 从PPO_2025-01-15_22-54-14继续

